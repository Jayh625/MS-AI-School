{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch를 사용하여 소프트맥스 회귀 모델을 학습하고 예측하는 실습\n",
    "- Iris 데이터 셋 사용하여 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.1, random_state=777)\n",
    "\n",
    "# conver data to Pytorch tensor\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 데이터 로더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "# TensorDataset() : 동일한 크기를 가진 텐서들을 첫번째 차원을 기준으로 결합해서 데이터셋 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression(nn.Module) :\n",
    "    def __init__(self, input_size, num_classes) :\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "num_classes = 3\n",
    "lr = 0.01\n",
    "num_epochs = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 옵티마이저 loss 함수 모델 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxRegression(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [10/14], Loss : 1.8770\n",
      "Epoch [2/100], Step [10/14], Loss : 1.6420\n",
      "Epoch [3/100], Step [10/14], Loss : 1.1396\n",
      "Epoch [4/100], Step [10/14], Loss : 1.0545\n",
      "Epoch [5/100], Step [10/14], Loss : 1.0427\n",
      "Epoch [6/100], Step [10/14], Loss : 0.9595\n",
      "Epoch [7/100], Step [10/14], Loss : 0.7893\n",
      "Epoch [8/100], Step [10/14], Loss : 0.8269\n",
      "Epoch [9/100], Step [10/14], Loss : 0.7735\n",
      "Epoch [10/100], Step [10/14], Loss : 0.7750\n",
      "Epoch [11/100], Step [10/14], Loss : 0.6788\n",
      "Epoch [12/100], Step [10/14], Loss : 0.6497\n",
      "Epoch [13/100], Step [10/14], Loss : 0.6443\n",
      "Epoch [14/100], Step [10/14], Loss : 0.5863\n",
      "Epoch [15/100], Step [10/14], Loss : 0.6546\n",
      "Epoch [16/100], Step [10/14], Loss : 0.5338\n",
      "Epoch [17/100], Step [10/14], Loss : 0.7051\n",
      "Epoch [18/100], Step [10/14], Loss : 0.5142\n",
      "Epoch [19/100], Step [10/14], Loss : 0.5187\n",
      "Epoch [20/100], Step [10/14], Loss : 0.6290\n",
      "Epoch [21/100], Step [10/14], Loss : 0.5931\n",
      "Epoch [22/100], Step [10/14], Loss : 0.5250\n",
      "Epoch [23/100], Step [10/14], Loss : 0.4234\n",
      "Epoch [24/100], Step [10/14], Loss : 0.4077\n",
      "Epoch [25/100], Step [10/14], Loss : 0.3820\n",
      "Epoch [26/100], Step [10/14], Loss : 0.4805\n",
      "Epoch [27/100], Step [10/14], Loss : 0.5077\n",
      "Epoch [28/100], Step [10/14], Loss : 0.5445\n",
      "Epoch [29/100], Step [10/14], Loss : 0.5671\n",
      "Epoch [30/100], Step [10/14], Loss : 0.5178\n",
      "Epoch [31/100], Step [10/14], Loss : 0.4105\n",
      "Epoch [32/100], Step [10/14], Loss : 0.4852\n",
      "Epoch [33/100], Step [10/14], Loss : 0.3635\n",
      "Epoch [34/100], Step [10/14], Loss : 0.6089\n",
      "Epoch [35/100], Step [10/14], Loss : 0.3768\n",
      "Epoch [36/100], Step [10/14], Loss : 0.4631\n",
      "Epoch [37/100], Step [10/14], Loss : 0.3586\n",
      "Epoch [38/100], Step [10/14], Loss : 0.4393\n",
      "Epoch [39/100], Step [10/14], Loss : 0.5194\n",
      "Epoch [40/100], Step [10/14], Loss : 0.4072\n",
      "Epoch [41/100], Step [10/14], Loss : 0.3151\n",
      "Epoch [42/100], Step [10/14], Loss : 0.4494\n",
      "Epoch [43/100], Step [10/14], Loss : 0.4528\n",
      "Epoch [44/100], Step [10/14], Loss : 0.3927\n",
      "Epoch [45/100], Step [10/14], Loss : 0.5748\n",
      "Epoch [46/100], Step [10/14], Loss : 0.5136\n",
      "Epoch [47/100], Step [10/14], Loss : 0.3908\n",
      "Epoch [48/100], Step [10/14], Loss : 0.4575\n",
      "Epoch [49/100], Step [10/14], Loss : 0.3233\n",
      "Epoch [50/100], Step [10/14], Loss : 0.3908\n",
      "Epoch [51/100], Step [10/14], Loss : 0.5097\n",
      "Epoch [52/100], Step [10/14], Loss : 0.3387\n",
      "Epoch [53/100], Step [10/14], Loss : 0.4370\n",
      "Epoch [54/100], Step [10/14], Loss : 0.5159\n",
      "Epoch [55/100], Step [10/14], Loss : 0.2681\n",
      "Epoch [56/100], Step [10/14], Loss : 0.3281\n",
      "Epoch [57/100], Step [10/14], Loss : 0.4544\n",
      "Epoch [58/100], Step [10/14], Loss : 0.4126\n",
      "Epoch [59/100], Step [10/14], Loss : 0.3415\n",
      "Epoch [60/100], Step [10/14], Loss : 0.4619\n",
      "Epoch [61/100], Step [10/14], Loss : 0.3619\n",
      "Epoch [62/100], Step [10/14], Loss : 0.3950\n",
      "Epoch [63/100], Step [10/14], Loss : 0.3322\n",
      "Epoch [64/100], Step [10/14], Loss : 0.3630\n",
      "Epoch [65/100], Step [10/14], Loss : 0.3167\n",
      "Epoch [66/100], Step [10/14], Loss : 0.5564\n",
      "Epoch [67/100], Step [10/14], Loss : 0.1750\n",
      "Epoch [68/100], Step [10/14], Loss : 0.2972\n",
      "Epoch [69/100], Step [10/14], Loss : 0.3189\n",
      "Epoch [70/100], Step [10/14], Loss : 0.4278\n",
      "Epoch [71/100], Step [10/14], Loss : 0.4907\n",
      "Epoch [72/100], Step [10/14], Loss : 0.4052\n",
      "Epoch [73/100], Step [10/14], Loss : 0.4010\n",
      "Epoch [74/100], Step [10/14], Loss : 0.4128\n",
      "Epoch [75/100], Step [10/14], Loss : 0.2993\n",
      "Epoch [76/100], Step [10/14], Loss : 0.2674\n",
      "Epoch [77/100], Step [10/14], Loss : 0.4032\n",
      "Epoch [78/100], Step [10/14], Loss : 0.4180\n",
      "Epoch [79/100], Step [10/14], Loss : 0.2300\n",
      "Epoch [80/100], Step [10/14], Loss : 0.4757\n",
      "Epoch [81/100], Step [10/14], Loss : 0.4499\n",
      "Epoch [82/100], Step [10/14], Loss : 0.2913\n",
      "Epoch [83/100], Step [10/14], Loss : 0.3115\n",
      "Epoch [84/100], Step [10/14], Loss : 0.4154\n",
      "Epoch [85/100], Step [10/14], Loss : 0.2644\n",
      "Epoch [86/100], Step [10/14], Loss : 0.3320\n",
      "Epoch [87/100], Step [10/14], Loss : 0.4035\n",
      "Epoch [88/100], Step [10/14], Loss : 0.3882\n",
      "Epoch [89/100], Step [10/14], Loss : 0.2892\n",
      "Epoch [90/100], Step [10/14], Loss : 0.2938\n",
      "Epoch [91/100], Step [10/14], Loss : 0.4205\n",
      "Epoch [92/100], Step [10/14], Loss : 0.3300\n",
      "Epoch [93/100], Step [10/14], Loss : 0.3137\n",
      "Epoch [94/100], Step [10/14], Loss : 0.3293\n",
      "Epoch [95/100], Step [10/14], Loss : 0.2449\n",
      "Epoch [96/100], Step [10/14], Loss : 0.2945\n",
      "Epoch [97/100], Step [10/14], Loss : 0.3919\n",
      "Epoch [98/100], Step [10/14], Loss : 0.2242\n",
      "Epoch [99/100], Step [10/14], Loss : 0.3744\n",
      "Epoch [100/100], Step [10/14], Loss : 0.5235\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        output = model(inputs)\n",
    "        outputs = output.float()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print\n",
    "        if (i+1) % 10 == 0 :\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss : {loss.item():.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 93.33\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad() :\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    acc = (predicted == y_test).sum().item() / len(y_test)\n",
    "    print(f\"Test Accuracy : {acc*100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
